{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5454dd81-2dae-4a7d-aed0-7756acbcac5c",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec248f35-9b8e-4976-960b-b5a9d54023ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.5.1\n",
      "  Using cached pyspark-3.5.1-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9.7 (from pyspark==3.5.1)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.1\n",
      "Collecting findspark==2.0.1\n",
      "  Using cached findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
      "Using cached findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n",
      "Collecting pandas==2.2.2\n",
      "  Using cached pandas-2.2.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.23.2 (from pandas==2.2.2)\n",
      "  Using cached numpy-2.0.1-cp311-cp311-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/spark/lib/python3.11/site-packages (from pandas==2.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/spark/lib/python3.11/site-packages (from pandas==2.2.2) (2024.1)\n",
      "Collecting tzdata>=2022.7 (from pandas==2.2.2)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/spark/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.16.0)\n",
      "Using cached pandas-2.2.2-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Using cached numpy-2.0.1-cp311-cp311-macosx_14_0_arm64.whl (5.3 MB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Installing collected packages: tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.0.1 pandas-2.2.2 tzdata-2024.1\n",
      "Collecting mariadb==1.1.10\n",
      "  Using cached mariadb-1.1.10-cp311-cp311-macosx_11_0_arm64.whl\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/spark/lib/python3.11/site-packages (from mariadb==1.1.10) (24.1)\n",
      "Installing collected packages: mariadb\n",
      "Successfully installed mariadb-1.1.10\n",
      "Collecting kafka-python==2.0.2\n",
      "  Using cached kafka_python-2.0.2-py2.py3-none-any.whl.metadata (7.8 kB)\n",
      "Using cached kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "Installing collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.0.2\n",
      "Collecting redis==5.0.7\n",
      "  Using cached redis-5.0.7-py3-none-any.whl.metadata (9.3 kB)\n",
      "Using cached redis-5.0.7-py3-none-any.whl (252 kB)\n",
      "Installing collected packages: redis\n",
      "Successfully installed redis-5.0.7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#Install mariadb-connector-C before proceeding here.\n",
    "\n",
    "os.environ[\"PYSPARK_HADOOP_VERSION\"] = \"3\"\n",
    "\n",
    "!pip install pyspark==3.5.1\n",
    "!pip install findspark==2.0.1\n",
    "!pip install pandas==2.2.2\n",
    "!pip install mariadb==1.1.10\n",
    "!pip install kafka-python==2.0.2\n",
    "!pip install redis==5.0.7\n",
    "\n",
    "#In\n",
    "#Set the following environment variables\n",
    "# PYSPARK_PYTHON=\"python\"\n",
    "\n",
    "#JAVA_HOME to Java 17\n",
    "#Add JAVA_HOME to PATH\n",
    "\n",
    "#For Windows only\n",
    "#  1. Copy the hadoop folder under \"exercise-files\" to c:\\hadoop\n",
    "#  2. Add an environment variable HADOOP_HOME=c:\\hadoop\n",
    "#  3. Add c:\\hadoop\\bin to PATH\n",
    "#  4. Close the notebook and the command prompt. Reopen command prompt, restart notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd2f841-4546-4215-ac56-809f94c0f100",
   "metadata": {},
   "source": [
    "### Setup MariaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01cb4fe2-6121-4fd0-8a7d-eb1b2ef47b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databases available : ['information_schema', 'mysql', 'performance_schema', 'spark_de', 'sys']\n",
      "Going to create warehouse_stock DB\n",
      "warehouse_stock DB successfully created\n",
      "Going to create global_stock DB\n",
      "globak_stock DB successfully created\n",
      "Going to create website_stats DB\n",
      "website_stats DB successfully created\n",
      "\n",
      "Schema and Tables:\n",
      "-----------------------------\n",
      "global_stock  :  item_stock\n",
      "website_stats  :  visit_stats\n",
      "warehouse_stock  :  item_stock\n"
     ]
    }
   ],
   "source": [
    "import mariadb\n",
    "\n",
    "#Connect to mariadb as root user\n",
    "root_conn = mariadb.connect(\n",
    "                user=\"root\",\n",
    "                password=\"spark\",\n",
    "                host=\"127.0.0.1\",\n",
    "                port=3306,\n",
    "                database=\"mysql\"\n",
    "            )\n",
    "\n",
    "db_cursor = root_conn.cursor()\n",
    "db_cursor.execute(\"SHOW DATABASES\")\n",
    "db_databases = db_cursor.fetchall()\n",
    "\n",
    "db_list=[]\n",
    "for database in db_databases:\n",
    "    db_list.append(database[0])\n",
    "    \n",
    "print(\"Databases available :\", db_list)\n",
    "\n",
    "#Create warehouse_stock database & item_stock table\n",
    "if ( \"warehouse_stock\" in db_list):\n",
    "    print(\"warehouse_stock DB already exists.\")\n",
    "else:\n",
    "    print(\"Going to create warehouse_stock DB\")\n",
    "    #Create DB\n",
    "    db_cursor.execute(\"CREATE DATABASE warehouse_stock\")\n",
    "    #Create table\n",
    "    db_cursor.execute(\"\"\"\n",
    "        CREATE TABLE `warehouse_stock`.`item_stock` (\n",
    "               `ID` INT NULL AUTO_INCREMENT,\n",
    "               `STOCK_DATE` DATETIME NOT NULL, \n",
    "               `WAREHOUSE_ID` VARCHAR(45) NOT NULL,\n",
    "               `ITEM_NAME` VARCHAR(45) NOT NULL,\n",
    "               `OPENING_STOCK` INT NOT NULL DEFAULT 0,\n",
    "               `RECEIPTS` INT  NOT NULL DEFAULT 0,\n",
    "               `ISSUES` INT  NOT NULL DEFAULT 0,\n",
    "               `UNIT_VALUE` DECIMAL(10,2)  NOT NULL DEFAULT 0,\n",
    "               PRIMARY KEY (`ID`),\n",
    "               INDEX `STOCK_DATE` (`STOCK_DATE` ASC));\"\"\")\n",
    "    \n",
    "    #Grant privileges to user spark\n",
    "    db_cursor.execute(\"GRANT ALL PRIVILEGES ON warehouse_stock.* to 'spark'@'%'\")\n",
    "    db_cursor.execute(\"FLUSH PRIVILEGES\")\n",
    "    print(\"warehouse_stock DB successfully created\")\n",
    "\n",
    "#Create global_stock database & item_stock table\n",
    "if ( \"global_stock\" in db_list):\n",
    "    print(\"global_stock DB already exists.\")\n",
    "else:\n",
    "    print(\"Going to create global_stock DB\")\n",
    "    #Create DB\n",
    "    db_cursor.execute(\"CREATE DATABASE global_stock\")\n",
    "    #Create table\n",
    "    db_cursor.execute(\"\"\"\n",
    "        CREATE TABLE `global_stock`.`item_stock` (\n",
    "                `ID` INT NULL AUTO_INCREMENT,\n",
    "                `STOCK_DATE` DATETIME NOT NULL,\n",
    "                `ITEM_NAME` VARCHAR(45) NOT NULL,\n",
    "                `TOTAL_REC` INT NOT NULL DEFAULT 0,\n",
    "                `OPENING_STOCK` INT NOT NULL DEFAULT 0,\n",
    "                `RECEIPTS` INT  NOT NULL DEFAULT 0,\n",
    "                `ISSUES` INT  NOT NULL DEFAULT 0,\n",
    "                `CLOSING_STOCK` INT NOT NULL DEFAULT 0,\n",
    "                `CLOSING_VALUE` DECIMAL(10,2)  NOT NULL DEFAULT 0,\n",
    "                PRIMARY KEY (`ID`),\n",
    "                INDEX `STOCK_DATE` (`STOCK_DATE` ASC));\"\"\")\n",
    "    \n",
    "    #Grant privileges to user spark\n",
    "    db_cursor.execute(\"GRANT ALL PRIVILEGES ON global_stock.* to 'spark'@'%'\")\n",
    "    db_cursor.execute(\"FLUSH PRIVILEGES\")\n",
    "    print(\"globak_stock DB successfully created\")\n",
    "\n",
    "#Create website_stats database & visit_stats table\n",
    "if ( \"website_stats\" in db_list):\n",
    "    print(\"website_stats DB already exists.\")\n",
    "else:\n",
    "    print(\"Going to create website_stats DB\")\n",
    "    #Create DB\n",
    "    db_cursor.execute(\"CREATE DATABASE website_stats\")\n",
    "    #Create table\n",
    "    db_cursor.execute(\"\"\"\n",
    "       CREATE TABLE `website_stats`.`visit_stats` (\n",
    "                `ID` int(11) NOT NULL AUTO_INCREMENT, \n",
    "                `INTERVAL_TIMESTAMP` DATETIME DEFAULT NULL,\n",
    "                `LAST_ACTION` varchar(45) DEFAULT NULL,\n",
    "                `DURATION` int(10) DEFAULT NULL, \n",
    "                PRIMARY KEY (`ID`));\"\"\")\n",
    "    \n",
    "    #Grant privileges to user spark\n",
    "    db_cursor.execute(\"GRANT ALL PRIVILEGES ON website_stats.* to 'spark'@'%'\")\n",
    "    db_cursor.execute(\"FLUSH PRIVILEGES\")\n",
    "    print(\"website_stats DB successfully created\")\n",
    "\n",
    "\n",
    "#Check if tables are created\n",
    "db_cursor.execute(\"\"\"\n",
    "    SELECT TABLE_SCHEMA, TABLE_NAME\n",
    "    FROM information_schema.tables\n",
    "    WHERE table_schema IN ('warehouse_stock','global_stock','website_stats')\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nSchema and Tables:\\n-----------------------------\")\n",
    "for schema, table in db_cursor:\n",
    "    print(schema,\" : \", table)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623511ad-c3e6-4827-b949-690b8335a959",
   "metadata": {},
   "source": [
    "### Setup Kafka Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8254c901-cead-47ad-ac81-65f8deee21d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating topics...\n",
      "Topics available now :\n",
      "--------------------------------\n",
      "spark.exercise.lastaction.long\n",
      "spark.streaming.carts.abandoned\n",
      "spark.streaming.website.visits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "consumer=KafkaConsumer(\n",
    "                group_id=\"setup\",\n",
    "                bootstrap_servers=\"localhost:9092\" )\n",
    "\n",
    "if \"spark.streaming.website.visits\" not in consumer.topics():\n",
    "\n",
    "    admin_client = KafkaAdminClient(\n",
    "                    bootstrap_servers=\"localhost:9092\", \n",
    "                    client_id='spark-de')\n",
    "    \n",
    "    topic_list = [\n",
    "        NewTopic(name=\"spark.streaming.website.visits\", \n",
    "                 num_partitions=1, replication_factor=1),\n",
    "        NewTopic(name=\"spark.streaming.carts.abandoned\", \n",
    "                 num_partitions=1, replication_factor=1),\n",
    "        NewTopic(name=\"spark.exercise.lastaction.long\", \n",
    "                 num_partitions=1, replication_factor=1)\n",
    "    ]\n",
    "\n",
    "    print(\"Creating topics...\")\n",
    "    result=admin_client.create_topics(new_topics=topic_list, validate_only=False)\n",
    "\n",
    "print(\"Topics available now :\\n--------------------------------\")\n",
    "for topic in consumer.topics():\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90edeb0b-17b1-470e-a215-bf0dd733a559",
   "metadata": {},
   "source": [
    "### Create a raw data folder to represent a distributed file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f33befba-98c6-4fb7-8a39-06002ffcf79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"./raw_data\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ceabd50-2599-4f29-bc60-3204b404c0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Library/Java/JavaVirtualMachines/jdk-17.jdk/Contents/Home\n",
      "/opt/anaconda3/envs/spark/bin:/opt/anaconda3/condabin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/opt/homebrew/bin:/usr/local/Homebrew/bin:/opt/stdlibs/homebrew\n"
     ]
    }
   ],
   "source": [
    "#Check if JAVA_HOME is set to Java 1.17\n",
    "print(os.environ[\"JAVA_HOME\"])\n",
    "\n",
    "#Check if HADOOP_HOME is set, needed for windows only\n",
    "#print(os.environ[\"HADOOP_HOME\"])\n",
    "\n",
    "#Check if JAVA_HOME & HADOOP_HOME (windows only) are in the PATH\n",
    "print(os.environ[\"PATH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da776ca-374f-4a29-95d2-764bce61f493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
